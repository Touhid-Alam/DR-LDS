{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2822650,"sourceType":"datasetVersion","datasetId":1715304},{"sourceId":733851,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":559285,"modelId":571858},{"sourceId":734248,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":559629,"modelId":572199}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================================================================================\n# L-DR-MEP: Latent Diabetic Retinopathy Markovian Entropy Processor\n# Resolution: 512x512 | Visualization: 5-Plot Grid (Classic) | Resume & Train\n# ==================================================================================\n\nimport os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.manifold import TSNE\nfrom skimage.metrics import structural_similarity as ssim\n\n# --- INSTALL DEPENDENCIES (Quiet Mode) ---\nprint(\"Installing dependencies...\")\nos.system(\"pip install -q diffusers transformers accelerate torchmetrics clean-fid\")\n\nfrom diffusers import UNet2DModel, DDIMScheduler, AutoencoderKL\n\n# ==================================================================================\n# 1. CONFIGURATION\n# ==================================================================================\nclass Config:\n    EXPERIMENT_NAME = \"L-DR-MEP_512_GridViz\"\n    \n    # --- DIMENSIONS ---\n    IMAGE_SIZE = 512       \n    LATENT_SIZE = 64        # 512 / 8 = 64\n    \n    # --- TRAINING SETTINGS ---\n    BATCH_SIZE = 4         \n    LEARNING_RATE = 1e-4\n    EPOCHS = 20             # Train for 20 ADDITIONAL epochs\n    \n    SAVE_INTERVAL = 5       \n    NUM_TIMESTEPS = 1000    \n    GRADIENT_ACCUMULATION = 2\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # --- PATHS ---\n    DATA_PATH = \"/kaggle/input/aptos2019/train_images/train_images\"\n    CSV_PATH = \"/kaggle/input/aptos2019/train_1.csv\" \n    \n    # Checkpoint to LOAD\n    LOAD_CHECKPOINT_PATH = \"/kaggle/input/ldr-40-512/pytorch/default/1/ldr_mep_512_finetuned.pth\"\n    \n    # Where to SAVE\n    OUTPUT_DIR = \"/kaggle/working/L-DR-MEP_Output\"\n    SAVE_CHECKPOINT_PATH = f\"{OUTPUT_DIR}/ldr_mep_512_finetuned.pth\"\n    \n    # Evaluation Directories\n    EVAL_DIR = f\"{OUTPUT_DIR}/evaluation_plots\"\n    PROGRESS_DIR = f\"{OUTPUT_DIR}/training_progress\"\n\nos.makedirs(Config.OUTPUT_DIR, exist_ok=True)\nos.makedirs(Config.EVAL_DIR, exist_ok=True)\nos.makedirs(Config.PROGRESS_DIR, exist_ok=True)\n\n# Path Verification\nif not os.path.exists(Config.CSV_PATH):\n    if os.path.exists(\"/kaggle/input/aptos2019/train.csv\"):\n        Config.CSV_PATH = \"/kaggle/input/aptos2019/train.csv\"\n\nprint(f\"‚öôÔ∏è Device: {Config.DEVICE}\")\n\n# ==================================================================================\n# 2. DATASET & PREPROCESSING\n# ==================================================================================\ndef crop_image_from_gray(img, tol=7):\n    if img.ndim == 2: mask = img > tol\n    else: mask = img[:,:,0] > tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef circle_crop_and_resize(img_path, size=512):\n    img = cv2.imread(img_path)\n    if img is None: return None\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (size, size))\n    height, width, _ = img.shape\n    x, y, r = int(width/2), int(height/2), int(np.amin((width/2, height/2)))\n    circle_img = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n    img = cv2.bitwise_and(img, img, mask=circle_img)\n    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0,0), 30), -4, 128)\n    return Image.fromarray(img)\n\nclass APTOSDataset(Dataset):\n    def __init__(self, csv_file, root_dir):\n        self.df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.id_col = self.df.columns[0] \n        self.label_col = self.df.columns[1]\n        self.transform = T.Compose([\n            T.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n            T.Normalize([0.5], [0.5])\n        ])\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        img_id = str(self.df.iloc[idx][self.id_col])\n        if not img_id.endswith(\".png\"): img_id += \".png\"\n        img_name = os.path.join(self.root_dir, img_id)\n        image = circle_crop_and_resize(img_name, size=Config.IMAGE_SIZE)\n        if image is None: image = Image.new('RGB', (Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n        return self.transform(image), int(self.df.iloc[idx][self.label_col])\n\nprint(\"Loading Data...\")\ndataset = APTOSDataset(Config.CSV_PATH, Config.DATA_PATH)\ndataloader = DataLoader(\n    dataset, \n    batch_size=Config.BATCH_SIZE, \n    shuffle=True, \n    num_workers=2,\n    pin_memory=True if Config.DEVICE == \"cuda\" else False,\n    prefetch_factor=2 if Config.DEVICE == \"cuda\" else None\n)\n\n# ==================================================================================\n# 3. MODEL SETUP (512x512 | Scaled Linear)\n# ==================================================================================\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(Config.DEVICE)\nvae.requires_grad_(False)\n\n# Deep U-Net for 64x64 Latents (128->256->512->512)\nmodel = UNet2DModel(\n    sample_size=Config.LATENT_SIZE, \n    in_channels=4,\n    out_channels=4,\n    layers_per_block=2,\n    block_out_channels=(128, 256, 512, 512), \n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n    class_embed_type=\"timestep\", \n    num_class_embeds=5\n).to(Config.DEVICE)\n\n# SCHEDULER: Scaled Linear (Optimum for 512px)\nscheduler = DDIMScheduler(\n    num_train_timesteps=Config.NUM_TIMESTEPS,\n    beta_schedule=\"scaled_linear\", \n    clip_sample=False,\n    prediction_type=\"epsilon\"\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n\n# ==================================================================================\n# 4. LOAD CHECKPOINT\n# ==================================================================================\ndef load_checkpoint():\n    start_epoch = 0\n    if os.path.exists(Config.LOAD_CHECKPOINT_PATH):\n        print(f\"üì• Loading Checkpoint from {Config.LOAD_CHECKPOINT_PATH}...\")\n        try:\n            checkpoint = torch.load(Config.LOAD_CHECKPOINT_PATH, map_location=Config.DEVICE)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            start_epoch = checkpoint['epoch'] + 1\n            print(f\"‚úÖ Successfully loaded. Resume from Global Epoch {start_epoch}.\")\n        except Exception as e:\n            print(f\"‚ùå Error loading: {e}. Starting from scratch.\")\n            start_epoch = 0\n    else:\n        print(f\"‚ö†Ô∏è No checkpoint found. Starting from scratch.\")\n    return start_epoch\n\ndef save_checkpoint(epoch, loss):\n    state = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(state, Config.SAVE_CHECKPOINT_PATH)\n    print(f\"üíæ Checkpoint saved at Epoch {epoch+1}\")\n\n# ==================================================================================\n# 5. VISUALIZATION: THE CLASSIC 5-PLOT GRID\n# ==================================================================================\n@torch.no_grad()\ndef generate_images(model, n_samples=1, class_label=2, steps=50, return_intermediates=False):\n    model.eval()\n    latents = torch.randn((n_samples, 4, Config.LATENT_SIZE, Config.LATENT_SIZE), device=Config.DEVICE)\n    labels = torch.tensor([class_label] * n_samples, device=Config.DEVICE)\n    scheduler.set_timesteps(steps)\n    \n    intermediates = []\n    # Capture exactly 5 steps: 0%, 25%, 50%, 75%, 100%\n    capture_indices = [0, int(steps*0.25), int(steps*0.50), int(steps*0.75), steps-1]\n    \n    for i, t in enumerate(scheduler.timesteps):\n        noise_pred = model(latents, t, class_labels=labels).sample\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n        \n        if return_intermediates and i in capture_indices:\n            decoded = vae.decode(latents / 0.18215).sample\n            decoded = (decoded / 2 + 0.5).clamp(0, 1).cpu()\n            intermediates.append(decoded)\n\n    final_images = vae.decode(latents / 0.18215).sample\n    final_images = (final_images / 2 + 0.5).clamp(0, 1).cpu()\n    \n    if return_intermediates:\n        return final_images, intermediates\n    return final_images\n\ndef plot_evolution_grid(epoch, title_prefix=\"Diffusion\"):\n    \"\"\"\n    Plots the classic 2-row, 5-column grid showing evolution at 0, 25, 50, 75, 100%\n    \"\"\"\n    classes_to_show = np.random.choice(range(5), 2, replace=False)\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    \n    for idx, cls in enumerate(classes_to_show):\n        _, intermediates = generate_images(model, n_samples=1, class_label=cls, steps=50, return_intermediates=True)\n        # intermediates is list of tensors [1, 3, 512, 512]\n        for step_idx, img_tensor in enumerate(intermediates):\n            img = img_tensor[0].permute(1, 2, 0).numpy()\n            axes[idx, step_idx].imshow(img)\n            axes[idx, step_idx].axis('off')\n            if idx == 0:\n                axes[idx, step_idx].set_title(f\"Step {step_idx * 25}%\")\n        axes[idx, 0].set_ylabel(f\"Class {cls}\", fontsize=12, fontweight='bold')\n    \n    plt.suptitle(f\"{title_prefix} Evolution (Global Epoch {epoch})\")\n    plt.tight_layout()\n    save_path = f\"{Config.PROGRESS_DIR}/epoch_{epoch}_evolution.png\"\n    plt.savefig(save_path)\n    plt.close()\n    print(f\"üñºÔ∏è Evolution Grid saved to {save_path}\")\n\n# ==================================================================================\n# 6. EXECUTION LOGIC\n# ==================================================================================\n\n# 1. Load Model\nstart_epoch = load_checkpoint()\n\n# 2. VALIDATION: Check the loaded model visually (Using 5-Plot Grid)\nprint(\"\\nüîé Validating loaded model (Pre-Train Visualization)...\")\nplot_evolution_grid(start_epoch, title_prefix=\"LOADED_MODEL_VALIDATION\")\n\n# 3. Training Loop\nend_epoch = start_epoch + Config.EPOCHS\nprint(f\"\\nüöÄ Resuming training from Epoch {start_epoch+1}.\")\nprint(f\"üîÑ Target: {Config.EPOCHS} additional epochs (End: {end_epoch}).\")\n\nlosses = []\n\nfor epoch in range(start_epoch, end_epoch):\n    model.train()\n    progress_bar = tqdm(dataloader, desc=f\"Global Epoch {epoch+1}/{end_epoch}\")\n    epoch_loss = 0\n    \n    for step, (images, labels) in enumerate(progress_bar):\n        images = images.to(Config.DEVICE, non_blocking=True)\n        labels = labels.to(Config.DEVICE, non_blocking=True)\n        \n        # VAE Encode\n        with torch.no_grad():\n            latents = vae.encode(images).latent_dist.sample() * 0.18215\n            \n        # Add Noise\n        noise = torch.randn_like(latents)\n        bs = latents.shape[0]\n        timesteps = torch.randint(0, Config.NUM_TIMESTEPS, (bs,), device=Config.DEVICE).long()\n        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n        \n        # Train\n        noise_pred = model(noisy_latents, timesteps, class_labels=labels).sample\n        loss = F.mse_loss(noise_pred, noise)\n        \n        loss.backward()\n        if (step + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        epoch_loss += loss.item()\n        progress_bar.set_postfix({\"Loss\": loss.item()})\n        \n    avg_loss = epoch_loss / len(dataloader)\n    losses.append(avg_loss)\n    \n    # Save/Plot every interval OR at the very last epoch\n    if (epoch + 1) % Config.SAVE_INTERVAL == 0 or (epoch + 1) == end_epoch:\n        save_checkpoint(epoch, avg_loss)\n        plot_evolution_grid(epoch + 1)\n\n# ==================================================================================\n# 7. FINAL EVALUATION\n# ==================================================================================\nprint(\"\\n‚úÖ Training Complete. Generating Metrics...\")\n\n# Plot 1: Loss\nif len(losses) > 0:\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(start_epoch + 1, end_epoch + 1), losses, marker='o', color='b')\n    plt.title(\"L-DR-MEP Training Loss\")\n    plt.grid(True)\n    plt.savefig(f\"{Config.EVAL_DIR}/1_Loss.png\")\n    plt.close()\n\n# Plot 2: XAI/SSIM\ntry:\n    fig, axes = plt.subplots(5, 3, figsize=(10, 15))\n    for cls in range(5):\n        if not any(dataset.df[dataset.label_col] == cls): continue\n        real_idx = dataset.df[dataset.df[dataset.label_col] == cls].index[0]\n        real_img, _ = dataset[real_idx]\n        real_img_np = (real_img.permute(1, 2, 0).numpy() + 1) / 2\n        \n        syn_tensor = generate_images(model, n_samples=1, class_label=cls)[0]\n        syn_img_np = syn_tensor.permute(1, 2, 0).numpy()\n        \n        # SSIM\n        real_gray = cv2.cvtColor((real_img_np*255).astype('uint8'), cv2.COLOR_RGB2GRAY)\n        syn_gray = cv2.cvtColor((syn_img_np*255).astype('uint8'), cv2.COLOR_RGB2GRAY)\n        score, diff = ssim(real_gray, syn_gray, full=True)\n        diff_heatmap = cv2.applyColorMap((diff * 255).astype(\"uint8\"), cv2.COLORMAP_JET)\n        \n        axes[cls, 0].imshow(real_img_np); axes[cls, 0].set_ylabel(f\"Class {cls}\")\n        axes[cls, 1].imshow(syn_img_np)\n        axes[cls, 2].imshow(diff_heatmap)\n        for j in range(3): axes[cls, j].axis('off')\n    plt.savefig(f\"{Config.EVAL_DIR}/3_XAI.png\"); plt.close()\n    print(\"‚úÖ XAI Plot Saved.\")\nexcept Exception as e: print(f\"Error in XAI: {e}\")\n\n# Plot 3: Spectral\nfig, axes = plt.subplots(5, 5, figsize=(15, 15))\nfor cls in range(5):\n    imgs = generate_images(model, n_samples=5, class_label=cls)\n    for i, img in enumerate(imgs):\n        gray = cv2.cvtColor((img.permute(1,2,0).numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        f = np.fft.fftshift(np.fft.fft2(gray))\n        spec = 20 * np.log(np.abs(f) + 1e-8)\n        axes[cls, i].imshow(spec, cmap='inferno'); axes[cls, i].axis('off')\nplt.savefig(f\"{Config.EVAL_DIR}/5_Spectral.png\"); plt.close()\nprint(\"‚úÖ Spectral Plot Saved.\")\n\n# Plot 4: t-SNE\nprint(\"Computing t-SNE...\")\nextractor = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\nextractor.fc = nn.Identity()\nextractor.to(Config.DEVICE)\nextractor.eval()\nfeatures = []\nlabels_list = []\n\n# Real Data\nfor cls in range(5):\n    idxs = dataset.df[dataset.df[dataset.label_col] == cls].index[:20]\n    for idx in idxs:\n        img, _ = dataset[idx]\n        with torch.no_grad():\n            feat = extractor(img.unsqueeze(0).to(Config.DEVICE)).cpu().numpy().flatten()\n        features.append(feat); labels_list.append(cls)\n\n# Syn Data\nfor cls in range(5):\n    imgs = generate_images(model, n_samples=20, class_label=cls, steps=30)\n    imgs = T.Normalize([-0.5], [2.0])(imgs).to(Config.DEVICE)\n    for i in range(20):\n        with torch.no_grad():\n            feat = extractor(imgs[i].unsqueeze(0)).cpu().numpy().flatten()\n        features.append(feat); labels_list.append(cls + 5)\n\nif len(features) > 0:\n    tsne = TSNE(n_components=2, perplexity=10, random_state=42)\n    embedded = tsne.fit_transform(np.array(features))\n    plt.figure(figsize=(10, 8))\n    colors = ['red', 'green', 'blue', 'orange', 'purple']\n    labels = [\"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative\"]\n    for cls in range(5):\n        mask = np.array(labels_list) == cls\n        if np.any(mask): plt.scatter(embedded[mask, 0], embedded[mask, 1], c=colors[cls], marker='o', alpha=0.6, label=f\"Real {labels[cls]}\")\n    for cls in range(5):\n        mask = np.array(labels_list) == (cls + 5)\n        if np.any(mask): plt.scatter(embedded[mask, 0], embedded[mask, 1], c=colors[cls], marker='*', s=100, label=f\"Syn {labels[cls]}\")\n    plt.legend()\n    plt.savefig(f\"{Config.EVAL_DIR}/6_tSNE.png\"); plt.close()\n\nprint(f\"\\n‚úÖ All Output in {Config.OUTPUT_DIR}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:07:48.546628Z","iopub.execute_input":"2026-01-28T14:07:48.547205Z","iopub.status.idle":"2026-01-28T19:30:50.138938Z","shell.execute_reply.started":"2026-01-28T14:07:48.547167Z","shell.execute_reply":"2026-01-28T19:30:50.137545Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n‚öôÔ∏è Device: cuda\nLoading Data...\nüì• Loading Checkpoint from /kaggle/input/ldr-40-512/pytorch/default/1/ldr_mep_512_finetuned.pth...\n‚úÖ Successfully loaded. Resume from Global Epoch 40.\n\nüîé Validating loaded model (Pre-Train Visualization)...\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_40_evolution.png\n\nüöÄ Resuming training from Epoch 41.\nüîÑ Target: 20 additional epochs (End: 60).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 41/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b4e06ad6c74865857ff7a7d142f19e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 42/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a8d5f9557b46f897b99a8f7ecf7731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 43/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad06243c032e44ecb87f3380ddcf9918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 44/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e303b312a6174853940adbd76790aafd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 45/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4efabd494b6f42158fc4a163cb2d9c32"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 45\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_45_evolution.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 46/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"921a5f4288cb4e42ba193c3838719f5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 47/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37b2a164cd84545b8b35c654f13210f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 48/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5aebb7dddc445de869ae8d46b9e6928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 49/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea02afe9357e459abafd3b3f1b578cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 50/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8b280b5b814950915718091b5658b2"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 50\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_50_evolution.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 51/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f8d7fa6be8442ea0f19c75af068b00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 52/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c11f7712ed431392c493381722eaf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 53/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a24cd81455d649f3b78c0a3e47285ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 54/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6bfe8bf2c749c3934de25f8a337706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 55/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa9d6cef2ad34b5290718ac501560911"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 55\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_55_evolution.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 56/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac77c308e074c4ab7807d97adcb5138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 57/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c34615ade6b4dbba5b985efb4a9c051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 58/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65faff78e00a44c4a617cb8f15cb3bd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 59/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf7a0979f9c9435bb8d9aafe5ff3a0c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 60/60:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49bbacc00c17408d9c9d15ba1785b712"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 60\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_60_evolution.png\n\n‚úÖ Training Complete. Generating Metrics...\n‚úÖ XAI Plot Saved.\n‚úÖ Spectral Plot Saved.\nComputing t-SNE...\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 176MB/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1736812849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;31m# Syn Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1736812849.py\u001b[0m in \u001b[0;36mgenerate_images\u001b[0;34m(model, n_samples, class_label, steps, return_intermediates)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mintermediates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mfinal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.18215\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mfinal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_images\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/utils/accelerate_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_hf_hook\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pre_forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_quant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mup_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# post-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mupsampler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.56 GiB is free. Process 3340 has 12.18 GiB memory in use. Of the allocated memory 6.55 GiB is allocated by PyTorch, and 5.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.56 GiB is free. Process 3340 has 12.18 GiB memory in use. Of the allocated memory 6.55 GiB is allocated by PyTorch, and 5.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":2}]}