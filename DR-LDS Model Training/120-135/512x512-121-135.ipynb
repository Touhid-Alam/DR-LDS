{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2822650,"sourceType":"datasetVersion","datasetId":1715304},{"sourceId":735352,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560577,"modelId":573162}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================================================================================\n# L-DR-MEP: Latent Diabetic Retinopathy Markovian Entropy Processor\n# Resolution: 512x512 | Visualization: 5-Plot Grid (Classic) | Resume & Train\n# ==================================================================================\n\nimport os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.manifold import TSNE\nfrom skimage.metrics import structural_similarity as ssim\n\n# --- INSTALL DEPENDENCIES (Quiet Mode) ---\nprint(\"Installing dependencies...\")\nos.system(\"pip install -q diffusers transformers accelerate torchmetrics clean-fid\")\n\nfrom diffusers import UNet2DModel, DDIMScheduler, AutoencoderKL\n\n# ==================================================================================\n# 1. CONFIGURATION\n# ==================================================================================\nclass Config:\n    EXPERIMENT_NAME = \"L-DR-MEP_512_GridViz\"\n    \n    # --- DIMENSIONS ---\n    IMAGE_SIZE = 512       \n    LATENT_SIZE = 64        # 512 / 8 = 64\n    \n    # --- TRAINING SETTINGS ---\n    BATCH_SIZE = 4         \n    LEARNING_RATE = 1e-4\n    EPOCHS = 15             # Train for 15 ADDITIONAL epochs\n    \n    SAVE_INTERVAL = 5       \n    NUM_TIMESTEPS = 1000    \n    GRADIENT_ACCUMULATION = 2\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # --- PATHS ---\n    DATA_PATH = \"/kaggle/input/aptos2019/train_images/train_images\"\n    CSV_PATH = \"/kaggle/input/aptos2019/train_1.csv\" \n    \n    # Checkpoint to LOAD\n    LOAD_CHECKPOINT_PATH = \"/kaggle/input/ldr-120/pytorch/default/1/ldr_mep_512_120.pth\"\n    \n    # Where to SAVE\n    OUTPUT_DIR = \"/kaggle/working/L-DR-MEP_Output\"\n    SAVE_CHECKPOINT_PATH = f\"{OUTPUT_DIR}/ldr_mep_512_finetuned.pth\"\n    \n    # Evaluation Directories\n    EVAL_DIR = f\"{OUTPUT_DIR}/evaluation_plots\"\n    PROGRESS_DIR = f\"{OUTPUT_DIR}/training_progress\"\n\nos.makedirs(Config.OUTPUT_DIR, exist_ok=True)\nos.makedirs(Config.EVAL_DIR, exist_ok=True)\nos.makedirs(Config.PROGRESS_DIR, exist_ok=True)\n\n# Path Verification\nif not os.path.exists(Config.CSV_PATH):\n    if os.path.exists(\"/kaggle/input/aptos2019/train.csv\"):\n        Config.CSV_PATH = \"/kaggle/input/aptos2019/train.csv\"\n\nprint(f\"‚öôÔ∏è Device: {Config.DEVICE}\")\n\n# ==================================================================================\n# 2. DATASET & PREPROCESSING\n# ==================================================================================\ndef crop_image_from_gray(img, tol=7):\n    if img.ndim == 2: mask = img > tol\n    else: mask = img[:,:,0] > tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef circle_crop_and_resize(img_path, size=512):\n    img = cv2.imread(img_path)\n    if img is None: return None\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = crop_image_from_gray(img)\n    img = cv2.resize(img, (size, size))\n    height, width, _ = img.shape\n    x, y, r = int(width/2), int(height/2), int(np.amin((width/2, height/2)))\n    circle_img = np.zeros((height, width), np.uint8)\n    cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n    img = cv2.bitwise_and(img, img, mask=circle_img)\n    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0,0), 30), -4, 128)\n    return Image.fromarray(img)\n\nclass APTOSDataset(Dataset):\n    def __init__(self, csv_file, root_dir):\n        self.df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.id_col = self.df.columns[0] \n        self.label_col = self.df.columns[1]\n        self.transform = T.Compose([\n            T.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n            T.Normalize([0.5], [0.5])\n        ])\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        img_id = str(self.df.iloc[idx][self.id_col])\n        if not img_id.endswith(\".png\"): img_id += \".png\"\n        img_name = os.path.join(self.root_dir, img_id)\n        image = circle_crop_and_resize(img_name, size=Config.IMAGE_SIZE)\n        if image is None: image = Image.new('RGB', (Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n        return self.transform(image), int(self.df.iloc[idx][self.label_col])\n\nprint(\"Loading Data...\")\ndataset = APTOSDataset(Config.CSV_PATH, Config.DATA_PATH)\ndataloader = DataLoader(\n    dataset, \n    batch_size=Config.BATCH_SIZE, \n    shuffle=True, \n    num_workers=2,\n    pin_memory=True if Config.DEVICE == \"cuda\" else False,\n    prefetch_factor=2 if Config.DEVICE == \"cuda\" else None\n)\n\n# ==================================================================================\n# 3. MODEL SETUP (512x512 | Scaled Linear)\n# ==================================================================================\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(Config.DEVICE)\nvae.requires_grad_(False)\n\n# Deep U-Net for 64x64 Latents (128->256->512->512)\nmodel = UNet2DModel(\n    sample_size=Config.LATENT_SIZE, \n    in_channels=4,\n    out_channels=4,\n    layers_per_block=2,\n    block_out_channels=(128, 256, 512, 512), \n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n    class_embed_type=\"timestep\", \n    num_class_embeds=5\n).to(Config.DEVICE)\n\n# SCHEDULER: Scaled Linear (Optimum for 512px)\nscheduler = DDIMScheduler(\n    num_train_timesteps=Config.NUM_TIMESTEPS,\n    beta_schedule=\"scaled_linear\", \n    clip_sample=False,\n    prediction_type=\"epsilon\"\n)\noptimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n\n# ==================================================================================\n# 4. LOAD CHECKPOINT\n# ==================================================================================\ndef load_checkpoint():\n    start_epoch = 0\n    if os.path.exists(Config.LOAD_CHECKPOINT_PATH):\n        print(f\"üì• Loading Checkpoint from {Config.LOAD_CHECKPOINT_PATH}...\")\n        try:\n            checkpoint = torch.load(Config.LOAD_CHECKPOINT_PATH, map_location=Config.DEVICE)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            start_epoch = checkpoint['epoch'] + 1\n            print(f\"‚úÖ Successfully loaded. Resume from Global Epoch {start_epoch}.\")\n        except Exception as e:\n            print(f\"‚ùå Error loading: {e}. Starting from scratch.\")\n            start_epoch = 0\n    else:\n        print(f\"‚ö†Ô∏è No checkpoint found. Starting from scratch.\")\n    return start_epoch\n\ndef save_checkpoint(epoch, loss):\n    state = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }\n    torch.save(state, Config.SAVE_CHECKPOINT_PATH)\n    print(f\"üíæ Checkpoint saved at Epoch {epoch+1}\")\n\n# ==================================================================================\n# 5. VISUALIZATION: THE CLASSIC 5-PLOT GRID\n# ==================================================================================\n@torch.no_grad()\ndef generate_images(model, n_samples=1, class_label=2, steps=50, return_intermediates=False):\n    model.eval()\n    latents = torch.randn((n_samples, 4, Config.LATENT_SIZE, Config.LATENT_SIZE), device=Config.DEVICE)\n    labels = torch.tensor([class_label] * n_samples, device=Config.DEVICE)\n    scheduler.set_timesteps(steps)\n    \n    intermediates = []\n    # Capture exactly 5 steps: 0%, 25%, 50%, 75%, 100%\n    capture_indices = [0, int(steps*0.25), int(steps*0.50), int(steps*0.75), steps-1]\n    \n    for i, t in enumerate(scheduler.timesteps):\n        noise_pred = model(latents, t, class_labels=labels).sample\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n        \n        if return_intermediates and i in capture_indices:\n            decoded = vae.decode(latents / 0.18215).sample\n            decoded = (decoded / 2 + 0.5).clamp(0, 1).cpu()\n            intermediates.append(decoded)\n\n    final_images = vae.decode(latents / 0.18215).sample\n    final_images = (final_images / 2 + 0.5).clamp(0, 1).cpu()\n    \n    if return_intermediates:\n        return final_images, intermediates\n    return final_images\n\ndef plot_evolution_grid(epoch, title_prefix=\"Diffusion\"):\n    \"\"\"\n    Plots the classic 2-row, 5-column grid showing evolution at 0, 25, 50, 75, 100%\n    \"\"\"\n    classes_to_show = np.random.choice(range(5), 2, replace=False)\n    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n    \n    for idx, cls in enumerate(classes_to_show):\n        _, intermediates = generate_images(model, n_samples=1, class_label=cls, steps=50, return_intermediates=True)\n        # intermediates is list of tensors [1, 3, 512, 512]\n        for step_idx, img_tensor in enumerate(intermediates):\n            img = img_tensor[0].permute(1, 2, 0).numpy()\n            axes[idx, step_idx].imshow(img)\n            axes[idx, step_idx].axis('off')\n            if idx == 0:\n                axes[idx, step_idx].set_title(f\"Step {step_idx * 25}%\")\n        axes[idx, 0].set_ylabel(f\"Class {cls}\", fontsize=12, fontweight='bold')\n    \n    plt.suptitle(f\"{title_prefix} Evolution (Global Epoch {epoch})\")\n    plt.tight_layout()\n    save_path = f\"{Config.PROGRESS_DIR}/epoch_{epoch}_evolution.png\"\n    plt.savefig(save_path)\n    plt.close()\n    print(f\"üñºÔ∏è Evolution Grid saved to {save_path}\")\n\n# ==================================================================================\n# 6. EXECUTION LOGIC\n# ==================================================================================\n\n# 1. Load Model\nstart_epoch = load_checkpoint()\n\n# 2. VALIDATION: Check the loaded model visually (Using 5-Plot Grid)\nprint(\"\\nüîé Validating loaded model (Pre-Train Visualization)...\")\nplot_evolution_grid(start_epoch, title_prefix=\"LOADED_MODEL_VALIDATION\")\n\n# 3. Training Loop\nend_epoch = start_epoch + Config.EPOCHS\nprint(f\"\\nüöÄ Resuming training from Epoch {start_epoch+1}.\")\nprint(f\"üîÑ Target: {Config.EPOCHS} additional epochs (End: {end_epoch}).\")\n\nlosses = []\n\nfor epoch in range(start_epoch, end_epoch):\n    model.train()\n    progress_bar = tqdm(dataloader, desc=f\"Global Epoch {epoch+1}/{end_epoch}\")\n    epoch_loss = 0\n    \n    for step, (images, labels) in enumerate(progress_bar):\n        images = images.to(Config.DEVICE, non_blocking=True)\n        labels = labels.to(Config.DEVICE, non_blocking=True)\n        \n        # VAE Encode\n        with torch.no_grad():\n            latents = vae.encode(images).latent_dist.sample() * 0.18215\n            \n        # Add Noise\n        noise = torch.randn_like(latents)\n        bs = latents.shape[0]\n        timesteps = torch.randint(0, Config.NUM_TIMESTEPS, (bs,), device=Config.DEVICE).long()\n        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n        \n        # Train\n        noise_pred = model(noisy_latents, timesteps, class_labels=labels).sample\n        loss = F.mse_loss(noise_pred, noise)\n        \n        loss.backward()\n        if (step + 1) % Config.GRADIENT_ACCUMULATION == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        epoch_loss += loss.item()\n        progress_bar.set_postfix({\"Loss\": loss.item()})\n        \n    avg_loss = epoch_loss / len(dataloader)\n    losses.append(avg_loss)\n    \n    # Save/Plot every interval OR at the very last epoch\n    if (epoch + 1) % Config.SAVE_INTERVAL == 0 or (epoch + 1) == end_epoch:\n        save_checkpoint(epoch, avg_loss)\n        plot_evolution_grid(epoch + 1)\n\n# ==================================================================================\n# 7. FINAL EVALUATION\n# ==================================================================================\nprint(\"\\n‚úÖ Training Complete. Generating Metrics...\")\n\n# Plot 1: Loss\nif len(losses) > 0:\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(start_epoch + 1, end_epoch + 1), losses, marker='o', color='b')\n    plt.title(\"L-DR-MEP Training Loss\")\n    plt.grid(True)\n    plt.savefig(f\"{Config.EVAL_DIR}/1_Loss.png\")\n    plt.close()\n\n# Plot 2: XAI/SSIM\ntry:\n    fig, axes = plt.subplots(5, 3, figsize=(10, 15))\n    for cls in range(5):\n        if not any(dataset.df[dataset.label_col] == cls): continue\n        real_idx = dataset.df[dataset.df[dataset.label_col] == cls].index[0]\n        real_img, _ = dataset[real_idx]\n        real_img_np = (real_img.permute(1, 2, 0).numpy() + 1) / 2\n        \n        syn_tensor = generate_images(model, n_samples=1, class_label=cls)[0]\n        syn_img_np = syn_tensor.permute(1, 2, 0).numpy()\n        \n        # SSIM\n        real_gray = cv2.cvtColor((real_img_np*255).astype('uint8'), cv2.COLOR_RGB2GRAY)\n        syn_gray = cv2.cvtColor((syn_img_np*255).astype('uint8'), cv2.COLOR_RGB2GRAY)\n        score, diff = ssim(real_gray, syn_gray, full=True)\n        diff_heatmap = cv2.applyColorMap((diff * 255).astype(\"uint8\"), cv2.COLORMAP_JET)\n        \n        axes[cls, 0].imshow(real_img_np); axes[cls, 0].set_ylabel(f\"Class {cls}\")\n        axes[cls, 1].imshow(syn_img_np)\n        axes[cls, 2].imshow(diff_heatmap)\n        for j in range(3): axes[cls, j].axis('off')\n    plt.savefig(f\"{Config.EVAL_DIR}/3_XAI.png\"); plt.close()\n    print(\"‚úÖ XAI Plot Saved.\")\nexcept Exception as e: print(f\"Error in XAI: {e}\")\n\n# Plot 3: Spectral\nfig, axes = plt.subplots(5, 5, figsize=(15, 15))\nfor cls in range(5):\n    imgs = generate_images(model, n_samples=5, class_label=cls)\n    for i, img in enumerate(imgs):\n        gray = cv2.cvtColor((img.permute(1,2,0).numpy() * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        f = np.fft.fftshift(np.fft.fft2(gray))\n        spec = 20 * np.log(np.abs(f) + 1e-8)\n        axes[cls, i].imshow(spec, cmap='inferno'); axes[cls, i].axis('off')\nplt.savefig(f\"{Config.EVAL_DIR}/5_Spectral.png\"); plt.close()\nprint(\"‚úÖ Spectral Plot Saved.\")\n\n# Plot 4: t-SNE\nprint(\"Computing t-SNE...\")\nextractor = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\nextractor.fc = nn.Identity()\nextractor.to(Config.DEVICE)\nextractor.eval()\nfeatures = []\nlabels_list = []\n\n# Real Data\nfor cls in range(5):\n    idxs = dataset.df[dataset.df[dataset.label_col] == cls].index[:20]\n    for idx in idxs:\n        img, _ = dataset[idx]\n        with torch.no_grad():\n            feat = extractor(img.unsqueeze(0).to(Config.DEVICE)).cpu().numpy().flatten()\n        features.append(feat); labels_list.append(cls)\n\n# Syn Data\nfor cls in range(5):\n    imgs = generate_images(model, n_samples=20, class_label=cls, steps=30)\n    imgs = T.Normalize([-0.5], [2.0])(imgs).to(Config.DEVICE)\n    for i in range(20):\n        with torch.no_grad():\n            feat = extractor(imgs[i].unsqueeze(0)).cpu().numpy().flatten()\n        features.append(feat); labels_list.append(cls + 5)\n\nif len(features) > 0:\n    tsne = TSNE(n_components=2, perplexity=10, random_state=42)\n    embedded = tsne.fit_transform(np.array(features))\n    plt.figure(figsize=(10, 8))\n    colors = ['red', 'green', 'blue', 'orange', 'purple']\n    labels = [\"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative\"]\n    for cls in range(5):\n        mask = np.array(labels_list) == cls\n        if np.any(mask): plt.scatter(embedded[mask, 0], embedded[mask, 1], c=colors[cls], marker='o', alpha=0.6, label=f\"Real {labels[cls]}\")\n    for cls in range(5):\n        mask = np.array(labels_list) == (cls + 5)\n        if np.any(mask): plt.scatter(embedded[mask, 0], embedded[mask, 1], c=colors[cls], marker='*', s=100, label=f\"Syn {labels[cls]}\")\n    plt.legend()\n    plt.savefig(f\"{Config.EVAL_DIR}/6_tSNE.png\"); plt.close()\n\nprint(f\"\\n‚úÖ All Output in {Config.OUTPUT_DIR}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-29T14:43:36.483487Z","iopub.execute_input":"2026-01-29T14:43:36.483906Z","iopub.status.idle":"2026-01-29T19:09:40.881892Z","shell.execute_reply.started":"2026-01-29T14:43:36.483881Z","shell.execute_reply":"2026-01-29T19:09:40.880482Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n","output_type":"stream"},{"name":"stderr","text":"2026-01-29 14:44:02.824190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769697842.987512      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769697843.033965      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769697843.419838      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769697843.419872      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769697843.419875      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769697843.419877      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚öôÔ∏è Device: cuda\nLoading Data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3133ca2dc9aa4c528a4c05bab364a34e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0777d5f9e0c843aaabeceb4ebd20bfab"}},"metadata":{}},{"name":"stdout","text":"üì• Loading Checkpoint from /kaggle/input/ldr-120/pytorch/default/1/ldr_mep_512_120.pth...\n‚úÖ Successfully loaded. Resume from Global Epoch 120.\n\nüîé Validating loaded model (Pre-Train Visualization)...\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_120_evolution.png\n\nüöÄ Resuming training from Epoch 121.\nüîÑ Target: 15 additional epochs (End: 135).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 121/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b8ac324a224c1f96bcd2b06d02bf09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 122/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6cff3b23f04194917c7b8ea9814e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 123/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ff7300de1945afa70eb339d4b0f74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 124/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae96cf7c7d304d32ab4f76b6c41106a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 125/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"642ca1f792ff467b9408108cc6e000e8"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 125\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_125_evolution.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 126/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a124932476164a44bfb3f5f6d1b02015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 127/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c07fef34f1634d999b0dd2b790757ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 128/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f77502bac934574a2b1b7a5b56d8c79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 129/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82babe03482344e99a9015ed12d6a487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 130/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f833dfa01cf4466e8f1a1bda6a37f8f5"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 130\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_130_evolution.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Global Epoch 131/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68ed6f66e23a484b8be43a0e831cda3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 132/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a27aa2bcb2496db99b1a27491560b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 133/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"295464c90da349e1a1200ebffb337026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 134/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbff5e666e346c8912e9c2060d4fd7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Global Epoch 135/135:   0%|          | 0/733 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"930260dbc2b44acbb88735e09ae6db3d"}},"metadata":{}},{"name":"stdout","text":"üíæ Checkpoint saved at Epoch 135\nüñºÔ∏è Evolution Grid saved to /kaggle/working/L-DR-MEP_Output/training_progress/epoch_135_evolution.png\n\n‚úÖ Training Complete. Generating Metrics...\n‚úÖ XAI Plot Saved.\n‚úÖ Spectral Plot Saved.\nComputing t-SNE...\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 175MB/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3920826100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;31m# Syn Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3920826100.py\u001b[0m in \u001b[0;36mgenerate_images\u001b[0;34m(model, n_samples, class_label, steps, return_intermediates)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mintermediates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mfinal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.18215\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mfinal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_images\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/utils/accelerate_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_hf_hook\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pre_forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/autoencoder_kl.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_quant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/autoencoders/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;31m# up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mup_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# post-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mupsampler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsamplers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, output_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_nearest3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.31 GiB is free. Process 3383 has 11.43 GiB memory in use. Of the allocated memory 5.74 GiB is allocated by PyTorch, and 5.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.31 GiB is free. Process 3383 has 11.43 GiB memory in use. Of the allocated memory 5.74 GiB is allocated by PyTorch, and 5.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":1}]}