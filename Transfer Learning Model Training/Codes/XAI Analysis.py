# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g4Cbvgi9wI0QktKu5VkOogehrkhsJPEn
"""

# --- 2. CONFIGURATION & DATA ---
DATA_ROOT = "/content/dataset/"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = "shufflenet_v2_ldr.pth"

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

train_loader = DataLoader(datasets.ImageFolder(os.path.join(DATA_ROOT, 'train'), transform), batch_size=32, shuffle=True)
val_dataset = datasets.ImageFolder(os.path.join(DATA_ROOT, 'val'), transform)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# --- 3. MODEL INITIALIZATION ---
model = models.shufflenet_v2_x1_0(weights='DEFAULT')
model.fc = nn.Linear(1024, 5) # ShuffleNet V2 uses 1024 features in the final FC layer
model = model.to(DEVICE)

# --- 4. TRAINING (20 EPOCHS) ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

print("ðŸš€ Starting 20-Epoch Training for ShuffleNet_V2...")
for epoch in range(20):
    model.train()
    running_loss = 0.0
    for imgs, lbls in tqdm(train_loader, desc=f"Epoch {epoch+1}/20"):
        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, lbls)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    # Validation check
    model.eval()
    correct = 0
    with torch.no_grad():
        for imgs, lbls in val_loader:
            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)
            preds = model(imgs).max(1)[1]
            correct += torch.sum(preds == lbls.data)

    acc = correct.double() / len(val_dataset)
    print(f"ðŸ“Š Epoch {epoch+1} - Loss: {running_loss/len(train_loader):.4f} | Val Acc: {acc:.4f}")

# --- 5. SAVE MODEL ---
torch.save(model.state_dict(), MODEL_PATH)
print(f"âœ… Model weights saved to {MODEL_PATH}")



import cv2
import numpy as np
import random
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.optim as optim
from tqdm import tqdm

# --- 1. XAI UTILITY FUNCTIONS ---
def generate_xai_maps(model, input_batch, target_class):
    input_batch.requires_grad = True
    model.zero_grad()

    # Store activations and gradients for Grad-CAM
    activations = None
    gradients = None

    def save_activations(module, input, output):
        nonlocal activations
        activations = output

    def save_gradients(module, grad_input, grad_output):
        nonlocal gradients
        gradients = grad_output[0] # grad_output is a tuple (grad_output_for_forward_pass_output,)

    # Register hooks on the target layer (model.conv5 in ShuffleNetV2)
    # model.conv5 is a Sequential module containing a Conv2d layer
    hook_handle_forward = model.conv5.register_forward_hook(save_activations)
    hook_handle_backward = model.conv5.register_full_backward_hook(save_gradients)

    # Forward Pass
    output = model(input_batch)

    # Perform backward pass for the target class
    # This will populate input_batch.grad for saliency and trigger the backward hook for Grad-CAM
    model.zero_grad() # Ensure model gradients are zeroed before this specific backward pass
    output[0, target_class].backward(retain_graph=True)

    # 1. Saliency Map
    saliency, _ = torch.max(input_batch.grad.data.abs(), dim=1)
    sal_map = saliency.squeeze().cpu().numpy()
    # Normalize saliency map for display/superimposition
    sal_map = (sal_map - sal_map.min()) / (sal_map.max() - sal_map.min() + 1e-8)

    # 2. Grad-CAM (Using captured activations and gradients)
    # Ensure hooks captured data
    if activations is None:
        raise RuntimeError("Failed to capture activations for Grad-CAM. Check hook registration and forward pass.")
    if gradients is None:
        raise RuntimeError("Failed to capture gradients for Grad-CAM. Check hook registration and backward pass.")

    # Average gradients spatially
    weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
    # Multiply weights with activations and sum along channel dimension
    cam = torch.sum(weights * activations, dim=1).squeeze().detach().cpu().numpy()
    cam = np.maximum(cam, 0) # Apply ReLU
    cam = cv2.resize(cam, (224, 224)) # Resize to input image dimensions
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8) # Normalize

    # Remove hooks to clean up
    hook_handle_forward.remove()
    hook_handle_backward.remove()

    # 3. Integrated Gradients Approximation
    ig_steps = 20
    # Initialize gradients accumulator for IG
    total_gradients_for_ig = torch.zeros_like(input_batch)
    baseline_tensor = torch.zeros_like(input_batch).to(DEVICE) # Assuming zero image is the baseline

    # Clear input gradients from saliency calculation to avoid contamination
    if input_batch.grad is not None:
        input_batch.grad.zero_()

    for i in range(ig_steps + 1):
        # Linear interpolation between baseline and input
        alpha = i / ig_steps
        interpolated_input = (baseline_tensor + alpha * (input_batch - baseline_tensor)).detach()
        interpolated_input.requires_grad = True

        model.zero_grad() # Clear model gradients before each backward pass
        ig_output = model(interpolated_input)
        ig_output[0, target_class].backward() # No retain_graph needed for the last backward in the loop

        if interpolated_input.grad is not None:
            total_gradients_for_ig += interpolated_input.grad.data

    # Compute Integrated Gradients: (input - baseline) * (average of gradients)
    ig_map_raw = (input_batch - baseline_tensor) * (total_gradients_for_ig / (ig_steps + 1))
    ig_map = torch.max(ig_map_raw.abs(), dim=1)[0].squeeze().detach().cpu().numpy()
    # Normalize IG map for display/superimposition
    ig_map = (ig_map - ig_map.min()) / (ig_map.max() - ig_map.min() + 1e-8)

    return cam, sal_map, ig_map

def generate_occlusion_map(model, input_batch, target_class, patch_size=16, stride=16, baseline_value=0.0):
    # input_batch is (1, C, H, W)
    C, H, W = input_batch.shape[1:]
    output_probs = model(input_batch).softmax(dim=1)
    original_score = output_probs[0, target_class].item()

    occlusion_map = np.zeros((H, W))
    count_map = np.zeros((H, W))

    # Create a baseline patch (e.g., black patch)
    baseline_patch = torch.full((C, patch_size, patch_size), baseline_value).to(input_batch.device)

    for i in range(0, H - patch_size + 1, stride):
        for j in range(0, W - patch_size + 1, stride):
            # Create an occluded image
            occluded_input = input_batch.clone()
            occluded_input[:, :, i:i+patch_size, j:j+patch_size] = baseline_patch

            # Get the new score
            occluded_output_probs = model(occluded_input).softmax(dim=1)
            occluded_score = occluded_output_probs[0, target_class].item()

            # Calculate the drop in score
            score_drop = original_score - occluded_score

            # Accumulate score drop in the occlusion map
            occlusion_map[i:i+patch_size, j:j+patch_size] += score_drop
            count_map[i:i+patch_size, j:j+patch_size] += 1

    # Average the scores where patches overlap
    occlusion_map /= (count_map + 1e-8) # Add small epsilon to avoid division by zero
    occlusion_map = np.maximum(0, occlusion_map) # Apply ReLU
    occlusion_map = (occlusion_map - occlusion_map.min()) / (occlusion_map.max() - occlusion_map.min() + 1e-8) # Normalize

    return occlusion_map

# --- 2. 5x6 PLOTTING ENGINE ---
CLASS_NAMES = ["No DR", "Mild", "Moderate", "Severe", "Proliferative"]
fig, axes = plt.subplots(5, 6, figsize=(24, 18)) # Adjusted to 6 columns and new figsize
plt.rcParams.update({'font.weight': 'bold', 'font.size': 12})

# Load the saved model
model.load_state_dict(torch.load(MODEL_PATH))
model.eval()

for row_idx in range(5):
    # RANDOMLY SELECT one image from the current class
    indices = [i for i, (_, label) in enumerate(val_dataset.imgs) if label == row_idx]
    rand_idx = random.choice(indices)
    img_tensor, label = val_dataset[rand_idx]
    input_batch = img_tensor.unsqueeze(0).to(DEVICE)

    g_cam, saliency, _ = generate_xai_maps(model, input_batch, row_idx) # Integrated Gradients removed from return
    occlusion_map = generate_occlusion_map(model, input_batch, row_idx)

    # Process original image for display
    orig = img_tensor.permute(1, 2, 0).numpy()
    orig = (orig * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])
    orig = np.clip(orig, 0, 1)

    # Superimposed Grad-CAM
    heatmap_gc = cv2.applyColorMap(np.uint8(255 * g_cam), cv2.COLORMAP_JET)
    heatmap_gc = cv2.cvtColor(heatmap_gc, cv2.COLOR_BGR2RGB) / 255.0
    superimposed_gc = cv2.addWeighted(orig.astype('float32'), 0.6, heatmap_gc.astype('float32'), 0.4, 0)

    # Superimposed Saliency
    heatmap_sal = cv2.applyColorMap(np.uint8(255 * saliency), cv2.COLORMAP_MAGMA)
    heatmap_sal = cv2.cvtColor(heatmap_sal, cv2.COLOR_BGR2RGB) / 255.0
    superimposed_sal = cv2.addWeighted(orig.astype('float32'), 0.6, heatmap_sal.astype('float32'), 0.4, 0)

    # Superimposed Occlusion
    heatmap_occ = cv2.applyColorMap(np.uint8(255 * occlusion_map), cv2.COLORMAP_VIRIDIS) # Using VIRIDIS for Occlusion
    heatmap_occ = cv2.cvtColor(heatmap_occ, cv2.COLOR_BGR2RGB) / 255.0
    superimposed_occlusion = cv2.addWeighted(orig.astype('float32'), 0.6, heatmap_occ.astype('float32'), 0.4, 0)

    # Vessel Map (Sobel Edge detection - Enhanced)
    gray = cv2.cvtColor((orig * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
    vessel = cv2.Sobel(gray, cv2.CV_64F, 1, 1, ksize=3)
    vessel_enhanced = np.abs(vessel)
    vessel_enhanced = (vessel_enhanced - vessel_enhanced.min()) / (vessel_enhanced.max() - vessel_enhanced.min() + 1e-8)

    # Column Mapping (6 columns now)
    images = [
        orig,
        superimposed_gc,
        superimposed_sal,
        superimposed_occlusion,
        g_cam, # Raw Grad-CAM as new column
        vessel_enhanced
    ]
    titles = [
        "Original",
        "Grad-CAM (Superimposed)",
        "Saliency (Superimposed)",
        "Occlusion (Superimposed)",
        "Raw Grad-CAM",
        "Vessel Map"
    ]

    for col_idx, data in enumerate(images):
        ax = axes[row_idx, col_idx]
        cmap = 'hot' if col_idx == 5 else ('jet' if col_idx == 4 else None) # Hot for Vessel Map, Jet for Raw Grad-CAM
        ax.imshow(data, cmap=cmap)
        ax.axis('off')
        if row_idx == 0: ax.set_title(titles[col_idx], fontsize=16, pad=10)
        if col_idx == 0: ax.text(-40, 112, CLASS_NAMES[row_idx], rotation=90, va='center', fontsize=18)

plt.tight_layout()
plt.savefig("ShuffleNet_XAI_5x6_Enhanced.png", dpi=300) # Changed filename to reflect 5x6 and enhanced
plt.show()